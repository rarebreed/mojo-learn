{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensor import Tensor, TensorShape, TensorSpec\n",
    "\n",
    "\n",
    "@register_passable\n",
    "struct Tensor1d[dtype: DType, size: Int](Stringable):\n",
    "    var reg: SIMD[dtype, size]\n",
    "\n",
    "    fn __init__(inout self):\n",
    "        self.reg = SIMD[dtype, size]()\n",
    "\n",
    "    fn __init__(inout self, val: SIMD[dtype, 1]):\n",
    "        self.reg = SIMD[dtype, size](val)\n",
    "\n",
    "    fn __init__(inout self, *val: SIMD[dtype, 1]):\n",
    "        self.reg = SIMD[dtype, size]()\n",
    "        var idx = 0\n",
    "        for i in range(len(val)):\n",
    "            if idx < size:\n",
    "                self.reg[i] = val[i]\n",
    "            idx += 1\n",
    "\n",
    "    fn __init__(inout self, reg: SIMD[dtype, size]):\n",
    "        self.reg = reg\n",
    "\n",
    "    fn __copyinit__(inout self, rhs: Self):\n",
    "        self.reg = rhs.reg\n",
    "\n",
    "    fn __add__(self, rhs: SIMD[dtype, 1]) -> Self:\n",
    "        \"\"\"Scalar add across elements.\"\"\"\n",
    "        var multiplier = SIMD[dtype, 1](1.0)\n",
    "        var new = self.reg.fma(multiplier, rhs)\n",
    "        return Self(new)\n",
    "\n",
    "    fn __sub__(self, rhs: SIMD[dtype, 1]) -> Self:\n",
    "        \"\"\"Scalar subtract across elements.\"\"\"\n",
    "        var multiplier = SIMD[dtype, 1](1.0)\n",
    "        var nrhs = rhs * -1\n",
    "        var new = self.reg.fma(multiplier, nrhs)\n",
    "        return Self(new)\n",
    "\n",
    "    fn __mul__(self, rhs: SIMD[dtype, 1]) -> Self:\n",
    "        var accum = SIMD[dtype, 1](0.0)\n",
    "        var new = self.reg.fma(rhs, accum)\n",
    "        return Self(new)\n",
    "\n",
    "    fn __str__(self) -> String:\n",
    "        return str(self.reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation function\n",
    "\n",
    "If you recall from calculus, a derivative is really the tangent line at a point on the graph.  The tangent is  \n",
    "calculated like a secant line of two points on the line, and the line connecting the two points.  But as the distance  \n",
    "betweem the two points gets smaller and smaller (ie, the delta x or |x1 - x2|) and approaches 0, we get the tangent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alias F64Tens = Tensor1d[DType.float64, 4]\n",
    "\n",
    "\n",
    "fn derive(\n",
    "    f: fn (F64Tens) -> F64Tens, \n",
    "    inp: F64Tens, \n",
    "    delta: Float64\n",
    ") -> F64Tens:\n",
    "    var f1 = f(inp + delta)\n",
    "    var f2 = f(inp - delta)\n",
    "    var num = Tensor1d(f1.reg - f2.reg)\n",
    "    var derivative = num.reg / ( 2 * delta)\n",
    "    return Tensor1d(derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculus derivative rules\n",
    "\n",
    "### Derivative of a log\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn func(data: F64Tens) -> F64Tens:\n",
    "    \"\"\"Equivalent of f(c) = 2x**3 + 3x**2.\n",
    "    \n",
    "    We are using no loops, just single instructions\n",
    "    \"\"\"\n",
    "    var x_3 = F64Tens(data.reg * data.reg * data.reg)\n",
    "    var two_x_3 = x_3 * 2\n",
    "    print(\"two_x_3 \",two_x_3)\n",
    "    var x_2 = F64Tens(data.reg * data.reg)\n",
    "    var three_x_2 = x_2 * 3\n",
    "    print(\"three_x_2\", three_x_2)\n",
    "    return F64Tens(two_x_3.reg + three_x_2.reg)\n",
    "\n",
    "var res = derive(func, F64Tens(1.0, 2.0, 3.0, 4.0), 0.00001)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Machine Learning learns\n",
    "\n",
    "In the above example, we knew that the function was 2x^3 + 3x^2.  We could have applied the derivative formula to do  \n",
    "the calculation.  But, what if we do not know what the function is?  Where the function is how we can calculate the  \n",
    "dependent variable from the independent variable?  \n",
    "\n",
    "In traditional programming, we have a machine that contains rules (algorithms) for how to operate on data, and this  \n",
    "gives us our ansers\n",
    "\n",
    "```bash\n",
    "                  +-----------+\n",
    "                  |           |\n",
    "----- data ------>| algorithm |----------> answers\n",
    "                  |           |\n",
    "                  +-----------+\n",
    "```\n",
    "\n",
    "In Machine learning, we don't know what the rules (algorithms) are.  Indeed, that is what we are trying to figure out.  \n",
    "In Supervised Learning, We have a set of inputs with their corresponding outputs (answers).  what we need to know, is  \n",
    "**how** they map to each other, so that given a new input, we can plug it into the rules to get a guesstimate.  \n",
    "\n",
    "In essence, we don't know how we got from the inputs to the outputs, so we can't just create he algoithm directly in  \n",
    "code.  What machine learning is learning, is how to create that algorithm or mapping.  Initially, the network will have  \n",
    "weights that probably do not predict the given inputs to the known output very well.  So we calculate a cost based on  \n",
    "a function that determines how far off the predicted value that the network came up with from the actual known value.  \n",
    "\n",
    "Then, a process called back propagation is called that adjusts the weights of the layers to minimize the loss.  This  \n",
    "happens iteratively so that (as long as we dont get stuck in a local minima) ideally, we get better and better  \n",
    "precitions.\n",
    "\n",
    "```bash\n",
    "                       +------------ adjust <-----------------+\n",
    "                       V                                      |\n",
    "                  +----+-----+                                |    \n",
    "                  |          |                                |\n",
    "----- data ------>|    ?     |-----> prediction ---> loss ----^\n",
    "                  |          |\n",
    "                  +----------+\n",
    "```\n",
    "\n",
    "### What is the ?\n",
    "\n",
    "So far, we talked about weights and how they get adjusted to minimize the loss (the predicted value vs the actual  \n",
    "value) but what exactly is in that black box in the 2nd diagram?\n",
    "\n",
    "That is what makes up your network (aka layers).  The network is a mathematical model that approximates the unknown  \n",
    "algorithm.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mojo",
   "language": "mojo",
   "name": "mojo-jupyter-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "mojo"
   },
   "file_extension": ".mojo",
   "mimetype": "text/x-mojo",
   "name": "mojo"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
